\chapter{Contribution} \label{chap:contribution}

Even though much work has been done according to the discussion in the previous chapter. These related work may not preciously reflect the fast pace of march in the field of hardware. The overall computing power is continuously advancing. In the case of Internet of Things, power of nodes deployed in various scenarios also boosts without hesitation. Tasks that used to be not suitable for nodes in the Internet of Things now become common ones. 

Based on this fact, I propose an aggressive model where more tasks are offloaded from cloud nodes to fog nodes to further lower network latency. Due to vague outcome of this adjustment, performance of various aspects of the system will be measured. As a result, a matrix should form to prepare raw data for further analyse.

On the other hand, Fog nodes in this architecture are not to substitute the Cloud Nodes completely. Considering the constrained power consumption, low unit computing price of cloud and elasticity of cloud infrastructure, tasks failing to be digested in the Fog layer will be overflowed to the cloud. So a relevant mechanism is drafted to dynamically allocate the tasks among Fog Nodes and Cloud Nodes

My contribution to this topic can be split into three parts:
\begin{itemize}
    \item A Fog Computing model with a prototype where face identification tasks are finished closer to end devices. The prototype of this model is implemented to evaluate the performance.
    \item A draft of dynamic allocation mechanism to judge where the face identification should be executed.
   \item A form of a transparent computing model based on the combination of the Cloud and Fog Nodes.
\end{itemize}

\section{Fog Computing based Face Identification}
As we discussed, the power of Fog nodes is increasing so that it can handle more computing tasks rather than just pre-processing tasks. In the architecture, computing of face detection and recognition is finished in the Fog Nodes. When these nodes find follow-up workload is beyond their capability, a dynamic allocation mechanism will be triggered to coordinate schedules of ambient Fog nodes or involved Cloud nodes. 

\section{Performance Evaluation of current computing model}
Considering that offloading workload from Cloud Nodes to Fog Nodes may lead to changes on various grounds, performance benchmarks should be set up to evaluate whether the merits outweigh the demerits.

\section{Dynamic Allocation Mechanism}
As mentioned in the section above, there is an mechanism used to overcome the potential overload of Fog Nodes. This mechanism helps calculate the required number of Fog Nodes based on prior traffic and support where to locate the computing nodes. Proxy techniques are assumed to be used to direct the request from end devices to target computing nodes.

\section{Transparent Computing Model}
The combination of Fog Nodes and Cloud Nodes in the architecture leads to transparent computing model. Since the dynamic allocation mechanism judge the location of the computing, end devices have no idea about where the requests are sent to. They can focus on their own business, and the feature of transparency is formed.