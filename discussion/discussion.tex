\chapter{Discussion} \label{chap:discussion}
This chapter spreads the discussion base on the performance of the implementation. The underlying logic of introducing the cloud is explained as well as the merits and demerits found in the research.

In the section of Implementation, controlled experiment setting is prepared. Though no benchmark or test against these two mode, the underperformance can be exhibited by naked eyes. For example, in the Native mode where Haar Feature-base Cascade Classifier is put into the layer of native (Yellow area in the figure \ref{fig:android_application_layer}), the performance of the application is crap. Multi-faces detection drags the render of the user interface dramatically. What's more, since the code is compiled through NDK tools, the coding work flow cannot retain consistency compared to coding in pure Java or Kotlin.

Another mode called Local mode is also included in the controlled experiment setting. The outcome of this mode is even worse due to its out of memory expectation. In this mode, the images are transferred to the application layer (Green area in the figure \ref{fig:android_application_layer}), namely Java Runtime first, then face detection is executed based it, contributing to more universal coding model. In other word, more developers or frameworks can be involved because we operate the images with Java or Kotlin code over the native layer.

The disappointing performance of previous two non-external-nodes-involved modes encourage the dependency of computing units outside. They reveal the reality that low power end devices in the network of Internet of Things have inability to handle CPU intensive tasks such as Face Detection even though the algorithm is optimised for low end hardware.

The performance of Fog Basic and Fog DNN mode are measured in the performance chapter and the difference is outstanding. Quality of the service is our initiative considering large scale deployment. If the quality of the service is acceptable, the value and reality of the system are proved.

To evaluate the quality of service, stable response time is introduced, which is the relatively constant response time by specific frequency of image collection. Frequency of image collection mimics the different pressure for the Fog Nodes in our case because the more image collected, the more face detection or recognition tasks are generated and sent to the Fog Nodes for computing. With adjusting the frequency, response time fluctuates accordingly. A thread-hold gets observed when the frequency drop to some degree, where requests with that frequency share similar response time. If you combine the result with the CPU usage, the percentage should be close to 100\% but a bit lower. So the stable response time indicate the ideal traffic of images towards that Fog Node.

The stable response time increases sharply when the face detection algorithm is replaced by face recognition algorithm, from 800 milliseconds(Fog Basic) to 5000 milliseconds(Fog DNN). Meanwhile ,the thread-hold of the traffic in Fog DNN is not ideal compared to that in Fog Basic (0.2 image per second vs 0.8 image per second). Heavy is the cost of deep metric learning supported face recognition feature.

Fog Cloud combination is the remedial solution. In the Fog Cloud mode (also called peak mode due to involvement of Cloud Nodes), unbearable tasks can be redirected to the Cloud Nodes to relief the pressure of the Fog Nodes. In this case, end devices share both benefits of Fog Nodes(low latency) and Cloud Nodes(elastic resource pool). 

In our system, the power of Cloud Nodes are assumed infinite. I achieve this goal by open sufficient AWS EC2 instances or powerful instance types ahead, so other risks are omitted currently such as orchestration of the Cloud Nodes.

Dynamic Allocation Mechanism helps judge where the requests should be assigned. The key features of it are as follows:
\begin{itemize}
    \item It calculate the capability of the Fog Node by history. (Stable response time as an indicator)
    \item It monitor the current traffic of the request and judge if Cloud Nodes are required.
    \item It overflow tasks to the Cloud Nodes by certain implementation of the proxy system.
\end{itemize}